#!/bin/bash
#PBS -N cacuo2-testU3-2

#PBS -q routing
#PBS -l cput=200:00:00
#PBS -l nodes=1:ppn=1:cluster

#PBS -m bea -M glaweh

# are we running on a cluster machine?
CLUSTER=$(echo $(/bin/hostname) | grep '^n')

# tmpdir for the calculations
TMPDIR="/local_scratch/glaweh/CaCuO2-testU3-2-bnd/"
EXTRACT="tar -C $TMPDIR -xjf /scratch/glaweh/CaCuO2-testU3-2/0000.tar.bz2"
# what to do with the TMPDIR afterwards
#CLEANUP="rm -rf $TMPDIR"
CLEANUP="collectdata.pl $TMPDIR"

# check for intermediate errors
error=0; trap "error=$((error|1))" ERR

# always change to the cluster home
if [ "$CLUSTER" ] ; then
	WD=$PBS_O_WORKDIR
	. /home/glaweh/cluster2/etc/bashrc
else
	WD=$(echo $PBS_O_WORKDIR | sed -e 's/^\/home\//\/net\/cluster\/users\//')
fi
cd $WD

## determine number of processors
NPROCS=`wc -w < $PBS_NODEFILE`
## write the nodes line-by-line to nodes.$PBS_JOBID
tr ' ' '
' < $PBS_NODEFILE | uniq > $WD/nodes.$PBS_JOBID
NNODES=`wc -l < $WD/nodes.$PBS_JOBID`

if [ "$NNODES" -gt "1" ] ; then
	## create tempdirs on each node
	for node in `<$WD/nodes.$PBS_JOBID` ; do
		ssh $node mkdir -p $TMPDIR
		[ "$EXTRACT" ] && ssh $node $EXTRACT
	done
else
	mkdir -p $TMPDIR
	$EXTRACT
fi

if [ "$NPROCS" -gt "1" ] ; then
	## start the mpi communication daemons
	mpdboot --totalnum=$NNODES --file=$WD/nodes.$PBS_JOBID
	RUN="mpiexec -np $NPROCS"
fi

## start a job on each processor
pw.x > CaCuO2.nscf.out < CaCuO2.nscf.in
bands.x > CaCuO2.bands.out < CaCuO2.bands.in

## shut down the mpi communication daemons
[ "$NPROCS" -gt "1" ] && mpdallexit

if [ "$NNODES" -gt "1" ] ; then
	## collect data
	for node in `<$WD/nodes.$PBS_JOBID` ; do
		ssh $node $CLEANUP
	done

else
	$CLEANUP
fi

exit $error
